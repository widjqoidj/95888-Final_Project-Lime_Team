{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'openpyxl', '--quiet'])\n",
    "\n",
    "#Config \n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "MAX_PAGES   = 3\n",
    "SCRIPT_DIR  = os.getcwd()\n",
    "OUTPUT_FILE = os.path.join(SCRIPT_DIR, \"pittsburgh_events.csv\")\n",
    "\n",
    "def clean(text):\n",
    "    return \" \".join(text.split()) if text else \"N/A\"\n",
    "\n",
    "def get_text(el):\n",
    "    return clean(el.get_text()) if el else \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape PGH.events\n",
    "\n",
    "def scrape_pgh_event_price(event_url):\n",
    "    \"\"\"\n",
    "    Fetch an individual pgh.events detail page and extract the price.\n",
    "    Handles formats: \"$39.17\", \"$35.00 to $41.23\", \"Free\", etc.\n",
    "    \"\"\"\n",
    "    if not event_url or event_url == \"N/A\":\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        resp = requests.get(event_url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"      ✗ Price fetch failed: {e}\")\n",
    "        return \"N/A\"\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    full_text = soup.get_text(\" \")\n",
    "\n",
    "    # Strategy 1: dedicated price/ticket/cost elements\n",
    "    for sel in [\"[class*='price']\", \"[class*='ticket']\", \"[class*='cost']\", \"[class*='admission']\"]:\n",
    "        for el in soup.select(sel):\n",
    "            txt = el.get_text(\" \", strip=True)\n",
    "            if re.search(r'\\$[\\d,]+', txt):\n",
    "                # pull out range or single price cleanly\n",
    "                range_m = re.search(\n",
    "                    r'(\\$[\\d,]+(?:\\.\\d{1,2})?\\s*(?:to|-|–)\\s*\\$[\\d,]+(?:\\.\\d{1,2})?)',\n",
    "                    txt, re.IGNORECASE)\n",
    "                if range_m:\n",
    "                    return range_m.group(1).strip()\n",
    "                single_m = re.search(r'\\$[\\d,]+(?:\\.\\d{1,2})?', txt)\n",
    "                if single_m:\n",
    "                    return single_m.group(0)\n",
    "\n",
    "    # Strategy 2: scan full page text for range first, then single price\n",
    "    range_m = re.search(\n",
    "        r'(\\$[\\d,]+(?:\\.\\d{1,2})?\\s*(?:to|-|–)\\s*\\$[\\d,]+(?:\\.\\d{1,2})?)',\n",
    "        full_text, re.IGNORECASE)\n",
    "    if range_m:\n",
    "        return range_m.group(1).strip()\n",
    "\n",
    "    single_m = re.search(r'(\\$[\\d,]+(?:\\.\\d{1,2})?)', full_text)\n",
    "    if single_m:\n",
    "        return single_m.group(1)\n",
    "\n",
    "    free_m = re.search(r'\\bfree\\b', full_text, re.IGNORECASE)\n",
    "    if free_m:\n",
    "        return \"Free\"\n",
    "\n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "def scrape_pgh_events():\n",
    "    pgh_events = []\n",
    "    for page_num in range(1, MAX_PAGES + 1):\n",
    "        url = (\"https://pgh.events/\" if page_num == 1\n",
    "               else f\"https://pgh.events/?page={page_num}\")\n",
    "        print(f\"[pgh.events] Fetching page {page_num}: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  ✗ {e}\"); break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        day_blocks = soup.select(\"[class*='day-module--day']\")\n",
    "        if not day_blocks:\n",
    "            print(\"  ✗ No day blocks found.\"); break\n",
    "        print(f\"  ✓ {len(day_blocks)} day block(s) found.\")\n",
    "\n",
    "        for day in day_blocks:\n",
    "            day_time_el = day.select_one(\"time\")\n",
    "            day_date = day_time_el.get(\"datetime\", \"N/A\")[:10] if day_time_el else \"N/A\"\n",
    "\n",
    "            for card in day.select(\"[class*='event-module--event']\"):\n",
    "                name_el    = card.select_one(\"[class*='event-module--mainLink']\")\n",
    "                event_name = get_text(name_el)\n",
    "\n",
    "                link_el    = name_el if (name_el and name_el.name == \"a\") else card.select_one(\"a[href]\")\n",
    "                source_url = link_el[\"href\"] if link_el else \"N/A\"\n",
    "                if source_url != \"N/A\" and source_url.startswith(\"/\"):\n",
    "                    source_url = \"https://pgh.events\" + source_url\n",
    "\n",
    "                location = \"N/A\"\n",
    "                for p in card.select(\"p\"):\n",
    "                    if not p.get(\"class\"):\n",
    "                        txt = clean(p.get_text())\n",
    "                        if txt and txt != \"N/A\":\n",
    "                            location = txt; break\n",
    "\n",
    "                card_time_el = card.select_one(\"time\")\n",
    "                event_date   = day_date\n",
    "                event_time   = \"N/A\"\n",
    "                if card_time_el:\n",
    "                    raw_dt = card_time_el.get(\"datetime\", \"\")\n",
    "                    if raw_dt and \"T\" in raw_dt:\n",
    "                        try:\n",
    "                            dt = datetime.strptime(re.sub(r'[+-]\\d{4}$', '', raw_dt), \"%Y-%m-%dT%H:%M:%S\")\n",
    "                            event_date = dt.strftime(\"%Y-%m-%d\")\n",
    "                            event_time = dt.strftime(\"%I:%M %p\")\n",
    "                        except ValueError:\n",
    "                            event_date = raw_dt[:10]\n",
    "\n",
    "                # Step 1: quick check on the card HTML itself\n",
    "                price = \"N/A\"\n",
    "                price_el = card.select_one(\"[class*='price']\") or card.select_one(\"[class*='cost']\")\n",
    "                if price_el:\n",
    "                    price = get_text(price_el)\n",
    "\n",
    "                # Step 2: regex on card text (handles inline prices)\n",
    "                if price == \"N/A\":\n",
    "                    m = re.search(\n",
    "                        r'(\\$[\\d,]+(?:\\.\\d{1,2})?\\s*(?:to|-|–)\\s*\\$[\\d,]+(?:\\.\\d{1,2})?'\n",
    "                        r'|\\$[\\d,]+(?:\\.\\d{1,2})?|Free)',\n",
    "                        card.get_text(), re.IGNORECASE)\n",
    "                    if m:\n",
    "                        price = m.group(0)\n",
    "\n",
    "                # Step 3: follow the detail page — pgh.events hides price there\n",
    "                if price == \"N/A\" and source_url != \"N/A\":\n",
    "                    print(f\"    ↳ [{event_name[:40]}] fetching detail page for price...\")\n",
    "                    price = scrape_pgh_event_price(source_url)\n",
    "                    print(f\"      → price found: {price}\")\n",
    "                    time.sleep(0.8)\n",
    "\n",
    "                pgh_events.append({\n",
    "                    \"event_name\": event_name, \"date\": event_date,\n",
    "                    \"time\": event_time,       \"location\": location,\n",
    "                    \"price\": price,           \"source\": \"pgh.events\",\n",
    "                    \"url\": source_url,\n",
    "                })\n",
    "\n",
    "        print(f\"  → {len(pgh_events)} events so far.\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    print(f\"\\n[pgh.events] Total: {len(pgh_events)} events\\n\")\n",
    "    return pgh_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eventbrite Pre Scrape\n",
    "\n",
    "def parse_eventbrite_datetime(soup, raw_html):\n",
    "    # Strategy 1: <time datetime=\"...\">\n",
    "    time_el = soup.select_one(\"time[datetime]\")\n",
    "    if time_el:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(time_el.get(\"datetime\", \"\").replace(\"Z\", \"+00:00\"))\n",
    "            return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%I:%M %p\")\n",
    "        except ValueError: pass\n",
    "    # Strategy 2: JSON-LD structured data\n",
    "    for script in soup.select(\"script[type='application/ld+json']\"):\n",
    "        try:\n",
    "            data  = json.loads(script.string or \"\")\n",
    "            start = data.get(\"startDate\", \"\")\n",
    "            if start:\n",
    "                dt = datetime.fromisoformat(start.replace(\"Z\", \"+00:00\"))\n",
    "                return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%I:%M %p\")\n",
    "        except: continue\n",
    "    # Strategy 3: regex on raw HTML\n",
    "    iso = re.search(r'\"startDate\"\\s*:\\s*\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})', raw_html)\n",
    "    if iso:\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(iso.group(1))\n",
    "            return dt.strftime(\"%Y-%m-%d\"), dt.strftime(\"%I:%M %p\")\n",
    "        except: pass\n",
    "    # Strategy 4: human-readable text \n",
    "    date_pat = re.compile(\n",
    "        r'(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s*'\n",
    "        r'(January|February|March|April|May|June|July|August|September|October|November|December)'\n",
    "        r'\\s+(\\d{1,2})(?:,?\\s*(\\d{4}))?', re.IGNORECASE)\n",
    "    time_pat = re.compile(r'\\b(\\d{1,2}:\\d{2}\\s*(?:AM|PM))\\b', re.IGNORECASE)\n",
    "    text = soup.get_text(\" \")\n",
    "    event_date = event_time = \"N/A\"\n",
    "    dm = date_pat.search(text)\n",
    "    tm = time_pat.search(text)\n",
    "    if dm:\n",
    "        try:\n",
    "            dt = datetime.strptime(f\"{dm.group(1)} {dm.group(2)} {dm.group(3) or '2026'}\", \"%B %d %Y\")\n",
    "            event_date = dt.strftime(\"%Y-%m-%d\")\n",
    "        except: event_date = f\"{dm.group(1)} {dm.group(2)}, 2026\"\n",
    "    if tm: event_time = tm.group(1).upper().replace(\" \", \"\")\n",
    "    return event_date, event_time\n",
    "\n",
    "def parse_eventbrite_location(soup):\n",
    "    for sel in [\"[data-spec='venue-name']\", \"[class*='venue-name']\",\n",
    "                \"[class*='location-info__address']\", \"address\"]:\n",
    "        el = soup.select_one(sel)\n",
    "        if el:\n",
    "            txt = clean(el.get_text())\n",
    "            if txt and len(txt) < 100: return txt\n",
    "    candidates = [clean(el.get_text()) for el in soup.find_all([\"p\",\"span\",\"div\",\"address\"])\n",
    "                  if \"Pittsburgh\" in clean(el.get_text()) and 5 < len(clean(el.get_text())) < 80]\n",
    "    return min(candidates, key=len) if candidates else \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Eventbrite \n",
    "\n",
    "def scrape_eventbrite():\n",
    "    # Step 1: collect event URLs from listing pages\n",
    "    print(\"[Eventbrite] Step 1: Collecting event URLs...\")\n",
    "    eb_urls = []\n",
    "    for page_num in range(1, MAX_PAGES + 1):\n",
    "        url = (\"https://www.eventbrite.com/d/pa--pittsburgh/all-events/\"\n",
    "               if page_num == 1\n",
    "               else f\"https://www.eventbrite.com/d/pa--pittsburgh/all-events/?page={page_num}\")\n",
    "        print(f\"  Fetching listing page {page_num}\")\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  ✗ {e}\"); break\n",
    "        soup  = BeautifulSoup(response.text, \"html.parser\")\n",
    "        found = []\n",
    "        for a in soup.select(\"a[href*='/e/']\"):\n",
    "            href = a[\"href\"].split(\"?\")[0]\n",
    "            if href not in eb_urls and href not in found: found.append(href)\n",
    "        eb_urls.extend(found)\n",
    "        print(f\"  ✓ {len(found)} URLs found on page {page_num}.\")\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    # Step 2: fetch each event detail page\n",
    "    print(f\"\\n[Eventbrite] {len(eb_urls)} URLs. Fetching detail pages...\\n\")\n",
    "    eb_events = []\n",
    "    for i, event_url in enumerate(eb_urls):\n",
    "        print(f\"  [{i+1}/{len(eb_urls)}] {event_url}\")\n",
    "        try:\n",
    "            resp = requests.get(event_url, headers=HEADERS, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "        except: time.sleep(1); continue\n",
    "        detail     = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        name_el    = detail.select_one(\"h1\") or detail.select_one(\"[class*='event-title']\")\n",
    "        event_name = get_text(name_el)\n",
    "        event_date, event_time = parse_eventbrite_datetime(detail, resp.text)\n",
    "        location               = parse_eventbrite_location(detail)\n",
    "        price_el = detail.select_one(\"[class*='ticket-price']\") or detail.select_one(\"[class*='conversion-bar']\")\n",
    "        price    = get_text(price_el)\n",
    "        if price == \"N/A\":\n",
    "            m = re.search(r'(Free|\\$[\\d,.]+)', resp.text)\n",
    "            price = m.group(0).capitalize() if m else \"N/A\"\n",
    "        eb_events.append({\n",
    "            \"event_name\": event_name, \"date\": event_date,\n",
    "            \"time\": event_time,       \"location\": location,\n",
    "            \"price\": price,           \"source\": \"Eventbrite\",\n",
    "            \"url\": event_url,\n",
    "        })\n",
    "        time.sleep(1.2)\n",
    "\n",
    "    print(f\"\\n[Eventbrite] Total: {len(eb_events)} events\\n\")\n",
    "    return eb_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data\n",
    "\n",
    "MANUAL_LOCATION_FIXES = {\n",
    "    \"Eddy TheatreWoodland\":         \"Eddy Theatre\",\n",
    "    \"Wyndham Grand\":                \"Wyndham Grand Pittsburgh Downtown\",\n",
    "    \"The Circuit Center Hot Metal\": \"The Circuit Center\",\n",
    "    \"1139 Penn\":                    \"1139 Penn Ave\",\n",
    "}\n",
    "\n",
    "def extract_max_price(price_str):\n",
    "    \"\"\"\n",
    "    Given a price string like \"$35.32 to $41.23\", \"$10\", \"Free\", or \"N/A\",\n",
    "    returns the maximum numeric value as a float, or \"Free\" / \"N/A\" as-is.\n",
    "    \"\"\"\n",
    "    if not isinstance(price_str, str):\n",
    "        return \"N/A\"\n",
    "    p = price_str.strip()\n",
    "    if p.lower() in (\"n/a\", \"\", \"free\"):\n",
    "        return p.capitalize() if p.lower() == \"free\" else \"N/A\"\n",
    "    # Find all dollar amounts in the string\n",
    "    amounts = re.findall(r'\\$([\\d,]+(?:\\.\\d{1,2})?)', p)\n",
    "    if amounts:\n",
    "        values = [float(a.replace(\",\", \"\")) for a in amounts]\n",
    "        return max(values)\n",
    "    # If no $ sign but looks numeric\n",
    "    num = re.search(r'([\\d,]+(?:\\.\\d{1,2})?)', p)\n",
    "    if num:\n",
    "        return float(num.group(1).replace(\",\", \"\"))\n",
    "    return \"N/A\"\n",
    "\n",
    "def clean_location(loc):\n",
    "    if not isinstance(loc, str) or loc == \"N/A\": return loc\n",
    "    if not re.match(r'^\\d', loc):\n",
    "        loc = re.sub(r'([a-zA-Z])(\\d)', r'\\1', loc).strip()\n",
    "    loc = re.split(r'\\s+\\d{1,5}\\s+', loc)[0].strip()\n",
    "    loc = re.sub(r',?\\s*Pittsburgh.*$', '', loc, flags=re.IGNORECASE).strip()\n",
    "    loc = re.sub(r'\\s+(Road|Street|Ave|Avenue|Blvd|Boulevard|Drive|Lane|Way)$',\n",
    "                 '', loc, flags=re.IGNORECASE).strip()\n",
    "    return loc.strip(\" ,\") if loc else \"N/A\"\n",
    "\n",
    "def build_dataframe(all_events):\n",
    "    df = pd.DataFrame(all_events, columns=[\n",
    "        \"event_name\", \"date\", \"time\", \"location\", \"price\", \"source\", \"url\"\n",
    "    ])\n",
    "    df = df[df[\"event_name\"].str.strip().str.len() > 0]\n",
    "    df = df[df[\"event_name\"] != \"N/A\"]\n",
    "    df.drop_duplicates(subset=[\"event_name\", \"date\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    df = df.fillna(\"N/A\")\n",
    "    df[\"location\"]  = df[\"location\"].apply(clean_location)\n",
    "    df[\"location\"]  = df[\"location\"].replace(MANUAL_LOCATION_FIXES)\n",
    "    df[\"price\"]     = df[\"price\"].apply(lambda p: p.rstrip(\".\") if isinstance(p, str) else p)\n",
    "    df[\"max_price\"] = df[\"price\"].apply(extract_max_price)\n",
    "    return df\n",
    "\n",
    "def save_csv(df, path):\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{len(df)} events saved to {path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(df[[\"event_name\",\"date\",\"time\",\"location\",\"price\",\"max_price\",\"source\"]].to_string(index=False))\n",
    "\n",
    "def save_excel(df, path):\n",
    "    \"\"\"Save the dataframe to an Excel file with basic formatting.\"\"\"\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name=\"Events\")\n",
    "        ws = writer.sheets[\"Events\"]\n",
    "        # Auto-fit column widths\n",
    "        for col in ws.columns:\n",
    "            max_len = max((len(str(cell.value)) for cell in col if cell.value), default=10)\n",
    "            ws.column_dimensions[col[0].column_letter].width = min(max_len + 4, 60)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{len(df)} events saved to Excel: {path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(df[[\"event_name\",\"date\",\"time\",\"location\",\"price\",\"max_price\",\"source\"]].to_string(index=False))\n",
    "\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df.fillna(\"N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90106041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Main\n",
    "\n",
    "OUTPUT_EXCEL = os.path.join(SCRIPT_DIR, \"pittsburgh_events.xlsx\")\n",
    "\n",
    "def prompt_user():\n",
    "    cached_exists = os.path.exists(OUTPUT_FILE)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  Pittsburgh Date Night App — Lime Team\")\n",
    "    print(\"=\" * 60)\n",
    "    if cached_exists:\n",
    "        print(f\"\\n  Cached dataset found: {OUTPUT_FILE}\\n\")\n",
    "        print(\"  [1] Use cached data  (instant)\")\n",
    "        print(\"  [2] Download fresh data  ( ~3-5 minutes)\\n\")\n",
    "        while True:\n",
    "            choice = input(\"  Enter 1 or 2: \").strip()\n",
    "            if choice == \"1\": return False\n",
    "            elif choice == \"2\":\n",
    "                confirm = input(\"  Are you sure? (y/n): \").strip().lower()\n",
    "                return confirm == \"y\"\n",
    "            else: print(\"  Please enter 1 or 2.\")\n",
    "    else:\n",
    "        print(\"\\n  No cached data found. Fresh download required (~3-5 mins).\")\n",
    "        input(\"  Press Enter to start...\")\n",
    "        return True\n",
    "\n",
    "def main():\n",
    "    use_fresh = prompt_user()\n",
    "    if use_fresh:\n",
    "        print(\"\\n[Starting fresh scrape...]\\n\")\n",
    "        all_events = scrape_pgh_events() + scrape_eventbrite()\n",
    "        if not all_events:\n",
    "            print(\"No events collected.\"); return\n",
    "        df = build_dataframe(all_events)\n",
    "        df = clean_dataframe(df)\n",
    "    else:\n",
    "        print(f\"\\n[Loading cached data...]\\n\")\n",
    "        df = load_csv(OUTPUT_FILE)\n",
    "        df = clean_dataframe(df)\n",
    "    save_csv(df, OUTPUT_FILE)\n",
    "    save_excel(df, OUTPUT_EXCEL)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
